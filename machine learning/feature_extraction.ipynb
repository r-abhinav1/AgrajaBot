{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "222969cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2cc439",
   "metadata": {},
   "source": [
    "## Wav2Vec2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40b1b7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = \"pitt_corpus_processed\"\n",
    "FEATURES_DIR = os.path.join(DATA_DIR, \"features\")\n",
    "os.makedirs(FEATURES_DIR, exist_ok=True)\n",
    "\n",
    "MODEL_NAME = \"facebook/wav2vec2-base-960h\"   # can switch to larger model if GPU memory allows\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"âœ… Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17589513",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Wav2Vec2Model(\n",
       "  (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "    (conv_layers): ModuleList(\n",
       "      (0): Wav2Vec2GroupNormConvLayer(\n",
       "        (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "        (activation): GELUActivation()\n",
       "        (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "      )\n",
       "      (1-4): 4 x Wav2Vec2NoLayerNormConvLayer(\n",
       "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (5-6): 2 x Wav2Vec2NoLayerNormConvLayer(\n",
       "        (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (feature_projection): Wav2Vec2FeatureProjection(\n",
       "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (projection): Linear(in_features=512, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): Wav2Vec2Encoder(\n",
       "    (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "      (conv): ParametrizedConv1d(\n",
       "        768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "        (parametrizations): ModuleDict(\n",
       "          (weight): ParametrizationList(\n",
       "            (0): _WeightNorm()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (padding): Wav2Vec2SamePadLayer()\n",
       "      (activation): GELUActivation()\n",
       "    )\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x Wav2Vec2EncoderLayer(\n",
       "        (attention): Wav2Vec2SdpaAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): Wav2Vec2FeedForward(\n",
       "          (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "          (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor = Wav2Vec2Processor.from_pretrained(MODEL_NAME)\n",
    "model = Wav2Vec2Model.from_pretrained(MODEL_NAME).to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b90c4bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_wav2vec2_features(file_path):\n",
    "    \"\"\"Extract mean-pooled Wav2Vec2 embeddings for a single WAV file.\"\"\"\n",
    "    # Load waveform\n",
    "    speech_array, sr = torchaudio.load(file_path)\n",
    "    \n",
    "    # Resample if necessary\n",
    "    if sr != 16000:\n",
    "        speech_array = torchaudio.transforms.Resample(sr, 16000)(speech_array)\n",
    "        sr = 16000\n",
    "    \n",
    "    # Convert to mono\n",
    "    if speech_array.shape[0] > 1:\n",
    "        speech_array = torch.mean(speech_array, dim=0, keepdim=True)\n",
    "    \n",
    "    # Preprocess\n",
    "    inputs = processor(speech_array.squeeze().numpy(), sampling_rate=sr, return_tensors=\"pt\", padding=True)\n",
    "    with torch.no_grad():\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Mean pool over time dimension\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy().flatten()\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef0dc208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Extracting Wav2Vec2 embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6930 [00:00<?, ?it/s]c:\\Users\\admin\\Desktop\\major major 6\\major 6 final new\\agraja_env\\lib\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py:969: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6930/6930 [02:05<00:00, 55.04it/s]\n"
     ]
    }
   ],
   "source": [
    "metadata = pd.read_csv(os.path.join(DATA_DIR, \"segment_metadata.csv\"))\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "print(\"ðŸš€ Extracting Wav2Vec2 embeddings...\")\n",
    "\n",
    "for idx, row in tqdm(metadata.iterrows(), total=len(metadata)):\n",
    "    file_path = row[\"filepath\"]\n",
    "    label = row[\"label\"]\n",
    "    \n",
    "    try:\n",
    "        emb = extract_wav2vec2_features(file_path)\n",
    "        features.append(emb)\n",
    "        labels.append(label)\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error processing {file_path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6e79edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Feature extraction complete!\n",
      "Saved: pitt_corpus_processed\\features\\wav2vec2_features.npy\n",
      "Shape: (6930, 768)\n"
     ]
    }
   ],
   "source": [
    "# Save Feature Arrays\n",
    "# ============================================\n",
    "features_np = np.array(features)\n",
    "labels_np = np.array(labels)\n",
    "\n",
    "np.save(os.path.join(FEATURES_DIR, \"wav2vec2_features.npy\"), features_np)\n",
    "np.save(os.path.join(FEATURES_DIR, \"labels.npy\"), labels_np)\n",
    "\n",
    "# Also save a CSV (flattened embeddings)\n",
    "df = pd.DataFrame(features_np)\n",
    "df[\"label\"] = labels_np\n",
    "df.to_csv(os.path.join(FEATURES_DIR, \"wav2vec2_features.csv\"), index=False)\n",
    "\n",
    "print(f\"âœ… Feature extraction complete!\")\n",
    "print(f\"Saved: {os.path.join(FEATURES_DIR, 'wav2vec2_features.npy')}\")\n",
    "print(f\"Shape: {features_np.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62151ac1",
   "metadata": {},
   "source": [
    "## OpenSMILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b09463e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import opensmile\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41dae943",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"pitt_corpus_processed\"   # your preprocessed 10-sec audio\n",
    "FEATURES_DIR = os.path.join(DATA_DIR, \"features\")\n",
    "os.makedirs(FEATURES_DIR, exist_ok=True)\n",
    "\n",
    "# Initialize OpenSMILE with eGeMAPS\n",
    "smile = opensmile.Smile(\n",
    "    feature_set=opensmile.FeatureSet.eGeMAPS,\n",
    "    feature_level=opensmile.FeatureLevel.Functionals\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9e77664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Extracting OpenSMILE eGeMAPS features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6930/6930 [21:47<00:00,  5.30it/s]\n"
     ]
    }
   ],
   "source": [
    "audio_files = glob.glob(os.path.join(DATA_DIR, \"**/*.wav\"), recursive=True)\n",
    "\n",
    "features_list = []\n",
    "labels_list = []\n",
    "\n",
    "print(\"ðŸš€ Extracting OpenSMILE eGeMAPS features...\")\n",
    "\n",
    "for file_path in tqdm(audio_files):\n",
    "    try:\n",
    "        # Extract features\n",
    "        feat_df = smile.process_file(file_path)\n",
    "        feat_vector = feat_df.values.flatten()  # flatten to 1D vector\n",
    "        \n",
    "        # Append to list\n",
    "        features_list.append(feat_vector)\n",
    "        \n",
    "        # Label from parent folder\n",
    "        label = os.path.basename(os.path.dirname(file_path))\n",
    "        labels_list.append(label)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error processing {file_path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c9c6b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… eGeMAPS feature extraction complete!\n",
      "Shape: (6930, 88)\n",
      "Saved at: pitt_corpus_processed\\features\n"
     ]
    }
   ],
   "source": [
    "features_np = np.array(features_list)\n",
    "labels_np = np.array(labels_list)\n",
    "\n",
    "np.save(os.path.join(FEATURES_DIR, \"egemaps_features.npy\"), features_np)\n",
    "np.save(os.path.join(FEATURES_DIR, \"egemaps_labels.npy\"), labels_np)\n",
    "\n",
    "# Also save CSV for ML use\n",
    "df = pd.DataFrame(features_np)\n",
    "df[\"label\"] = labels_np\n",
    "df.to_csv(os.path.join(FEATURES_DIR, \"egemaps_features.csv\"), index=False)\n",
    "\n",
    "print(f\"âœ… eGeMAPS feature extraction complete!\")\n",
    "print(f\"Shape: {features_np.shape}\")\n",
    "print(f\"Saved at: {FEATURES_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72ac077",
   "metadata": {},
   "source": [
    "## WavLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a127d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\admin\\desktop\\major major 6\\major 6 final new\\agraja_env\\lib\\site-packages (4.42.2)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\admin\\desktop\\major major 6\\major 6 final new\\agraja_env\\lib\\site-packages (2.4.1+cu121)\n",
      "Requirement already satisfied: soundfile in c:\\users\\admin\\desktop\\major major 6\\major 6 final new\\agraja_env\\lib\\site-packages (0.13.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\admin\\desktop\\major major 6\\major 6 final new\\agraja_env\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\desktop\\major major 6\\major 6 final new\\agraja_env\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\admin\\desktop\\major major 6\\major 6 final new\\agraja_env\\lib\\site-packages (from transformers) (0.35.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in c:\\users\\admin\\desktop\\major major 6\\major 6 final new\\agraja_env\\lib\\site-packages (from transformers) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\desktop\\major major 6\\major 6 final new\\agraja_env\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\admin\\desktop\\major major 6\\major 6 final new\\agraja_env\\lib\\site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\admin\\desktop\\major major 6\\major 6 final new\\agraja_env\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\desktop\\major major 6\\major 6 final new\\agraja_env\\lib\\site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\admin\\desktop\\major major 6\\major 6 final new\\agraja_env\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\admin\\desktop\\major major 6\\major 6 final new\\agraja_env\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: torch==2.4.1+cu121 in c:\\users\\admin\\desktop\\major major 6\\major 6 final new\\agraja_env\\lib\\site-packages (from torchaudio) (2.4.1+cu121)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\admin\\desktop\\major major 6\\major 6 final new\\agraja_env\\lib\\site-packages (from torch==2.4.1+cu121->torchaudio) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\admin\\desktop\\major major 6\\major 6 final new\\agraja_env\\lib\\site-packages (from torch==2.4.1+cu121->torchaudio) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\admin\\desktop\\major major 6\\major 6 final new\\agraja_env\\lib\\site-packages (from torch==2.4.1+cu121->torchaudio) (3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\desktop\\major major 6\\major 6 final new\\agraja_env\\lib\\site-packages (from torch==2.4.1+cu121->torchaudio) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\admin\\desktop\\major major 6\\major 6 final new\\agraja_env\\lib\\site-packages (from torch==2.4.1+cu121->torchaudio) (2024.6.1)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\admin\\desktop\\major major 6\\major 6 final new\\agraja_env\\lib\\site-packages (from soundfile) (1.17.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\desktop\\major major 6\\major 6 final new\\agraja_env\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: pycparser in c:\\users\\admin\\desktop\\major major 6\\major 6 final new\\agraja_env\\lib\\site-packages (from cffi>=1.0->soundfile) (2.23)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\admin\\desktop\\major major 6\\major 6 final new\\agraja_env\\lib\\site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\desktop\\major major 6\\major 6 final new\\agraja_env\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\desktop\\major major 6\\major 6 final new\\agraja_env\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\desktop\\major major 6\\major 6 final new\\agraja_env\\lib\\site-packages (from requests->transformers) (2025.10.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\desktop\\major major 6\\major 6 final new\\agraja_env\\lib\\site-packages (from jinja2->torch==2.4.1+cu121->torchaudio) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\admin\\desktop\\major major 6\\major 6 final new\\agraja_env\\lib\\site-packages (from sympy->torch==2.4.1+cu121->torchaudio) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torchaudio soundfile tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01c1b1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import WavLMModel, Wav2Vec2FeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "114f5117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of the model checkpoint at microsoft/wavlm-base were not used when initializing WavLMModel: ['encoder.pos_conv_embed.conv.weight_g', 'encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing WavLMModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing WavLMModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of WavLMModel were not initialized from the model checkpoint at microsoft/wavlm-base and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WavLMModel(\n",
       "  (feature_extractor): WavLMFeatureEncoder(\n",
       "    (conv_layers): ModuleList(\n",
       "      (0): WavLMGroupNormConvLayer(\n",
       "        (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "        (activation): GELUActivation()\n",
       "        (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "      )\n",
       "      (1-4): 4 x WavLMNoLayerNormConvLayer(\n",
       "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (5-6): 2 x WavLMNoLayerNormConvLayer(\n",
       "        (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (feature_projection): WavLMFeatureProjection(\n",
       "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (projection): Linear(in_features=512, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): WavLMEncoder(\n",
       "    (pos_conv_embed): WavLMPositionalConvEmbedding(\n",
       "      (conv): ParametrizedConv1d(\n",
       "        768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "        (parametrizations): ModuleDict(\n",
       "          (weight): ParametrizationList(\n",
       "            (0): _WeightNorm()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (padding): WavLMSamePadLayer()\n",
       "      (activation): GELUActivation()\n",
       "    )\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0): WavLMEncoderLayer(\n",
       "        (attention): WavLMAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (gru_rel_pos_linear): Linear(in_features=64, out_features=8, bias=True)\n",
       "          (rel_attn_embed): Embedding(320, 12)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): WavLMFeedForward(\n",
       "          (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "          (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1-11): 11 x WavLMEncoderLayer(\n",
       "        (attention): WavLMAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (gru_rel_pos_linear): Linear(in_features=64, out_features=8, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): WavLMFeedForward(\n",
       "          (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "          (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR = \"pitt_corpus_processed\"\n",
    "FEATURES_DIR = os.path.join(DATA_DIR, \"features\")\n",
    "os.makedirs(FEATURES_DIR, exist_ok=True)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "MODEL_NAME = \"microsoft/wavlm-base\"\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(MODEL_NAME)\n",
    "model = WavLMModel.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "abccf719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Extracting WavLM features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6930/6930 [02:48<00:00, 41.25it/s]\n"
     ]
    }
   ],
   "source": [
    "audio_files = glob.glob(os.path.join(DATA_DIR, \"**/*.wav\"), recursive=True)\n",
    "\n",
    "features_list = []\n",
    "labels_list = []\n",
    "\n",
    "print(\"ðŸš€ Extracting WavLM features...\")\n",
    "\n",
    "for file_path in tqdm(audio_files):\n",
    "    try:\n",
    "        # Load audio\n",
    "        waveform, sr = torchaudio.load(file_path)\n",
    "        waveform = waveform.mean(dim=0)  # Convert to mono if stereo\n",
    "        \n",
    "        # Resample if needed\n",
    "        if sr != 16000:\n",
    "            waveform = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)(waveform)\n",
    "        \n",
    "        # Tokenize\n",
    "        input_values = processor(waveform.numpy(), sampling_rate=16000, return_tensors=\"pt\").input_values\n",
    "        input_values = input_values.to(DEVICE)\n",
    "        \n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_values)\n",
    "            hidden_states = outputs.last_hidden_state  # shape: [1, seq_len, hidden_dim]\n",
    "        \n",
    "        # Aggregate over time dimension (mean pooling)\n",
    "        feat_vector = hidden_states.mean(dim=1).squeeze().cpu().numpy()\n",
    "        \n",
    "        features_list.append(feat_vector)\n",
    "        \n",
    "        # Label from parent folder\n",
    "        label = os.path.basename(os.path.dirname(file_path))\n",
    "        labels_list.append(label)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error processing {file_path}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "61daf391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… WavLM feature extraction complete!\n",
      "Shape: (6930, 768)\n",
      "Saved at: pitt_corpus_processed\\features\n"
     ]
    }
   ],
   "source": [
    "features_np = np.array(features_list)\n",
    "labels_np = np.array(labels_list)\n",
    "\n",
    "np.save(os.path.join(FEATURES_DIR, \"wavlm_features.npy\"), features_np)\n",
    "np.save(os.path.join(FEATURES_DIR, \"wavlm_labels.npy\"), labels_np)\n",
    "\n",
    "# Save CSV\n",
    "df = pd.DataFrame(features_np)\n",
    "df[\"label\"] = labels_np\n",
    "df.to_csv(os.path.join(FEATURES_DIR, \"wavlm_features.csv\"), index=False)\n",
    "\n",
    "print(f\"âœ… WavLM feature extraction complete!\")\n",
    "print(f\"Shape: {features_np.shape}\")\n",
    "print(f\"Saved at: {FEATURES_DIR}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agraja_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
